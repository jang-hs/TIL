(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{337:function(t,a,s){"use strict";s.r(a);var n=s(27),r=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"spark-조인-join-최적화"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-조인-join-최적화"}},[t._v("#")]),t._v(" Spark 조인(join) 최적화")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://medium.com/@harkiratk/spark-join-optimization-fe02a1fb72f4",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark: Join optimization"),a("OutboundLink")],1),t._v("를 번역하였습니다.")]),t._v(" "),a("p",[t._v("조인(join) 연산은 테이블을 결합하는 과정으로 비용이 많이 들며, 데이터 셔플(shuffle)과 성능 병목현상을 초래할 수 있습니다.")]),t._v(" "),a("h2",{attrs:{id:"조인을-수행하기-전에"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#조인을-수행하기-전에"}},[t._v("#")]),t._v(" 조인을 수행하기 전에")]),t._v(" "),a("h3",{attrs:{id:"데이터-제한-limit-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#데이터-제한-limit-data"}},[t._v("#")]),t._v(" 데이터 제한(Limit Data)")]),t._v(" "),a("p",[t._v("조인 작업 전에 DataFrame에서 불필요한 행(Row)과 열(Column)을 필터링하여 데이터를 줄이세요.")]),t._v(" "),a("ul",[a("li",[t._v("데이터 셔플 중 전송량 감소")]),t._v(" "),a("li",[t._v("Executor 처리 시간 단축")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("join_default "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" product"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 행 제한(Predicate Pushdown): 특정 조건에 맞는 행만 필터링")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 평점이 4 이상인 제품만 반환")]),t._v("\nproduct_4star "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" product"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\\\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("filter")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'rating'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 열 제한(Projection Pushdown): 필요한 열만 선택")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# catalog 데이터셋에서 product_id와 category 열만 스캔")]),t._v("\ncatalog_ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" catalog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'category'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\njoin_optimized "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" product_4star"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("catalog_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"catalyst-optimizer-활용"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#catalyst-optimizer-활용"}},[t._v("#")]),t._v(" Catalyst Optimizer 활용")]),t._v(" "),a("p",[t._v("고수준 Spark API 사용: DataFrame, Dataset, SparkSQL을 활용하세요. Dynamic Frame이나 RDD를 사용하는 경우 Catalyst Optimizer의 최적화를 적용할 수 없습니다.\n"),a("img",{attrs:{src:"https://miro.medium.com/v2/resize:fit:1400/1*-kbjC1s-8aDwAPFPUDc5GA.png",alt:""}}),t._v("\n적응형 쿼리 실행(AQE, Adaptive Query Execution): Catalyst Optimizer는 실행 중인 Spark 작업에서 런타임 통계를 활용하여 쿼리를 최적화합니다.")]),t._v(" "),a("h2",{attrs:{id:"broadcast-hash-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#broadcast-hash-join"}},[t._v("#")]),t._v(" Broadcast Hash Join")]),t._v(" "),a("p",[t._v("Broadcast Hash Join은 데이터 셔플(shuffling)이 필요 없으며, 작은 테이블과 큰 테이블을 조인할 때만 적용할 수 있습니다.")]),t._v(" "),a("p",[t._v("작은 테이블이 단일 Spark 실행기(executor)의 메모리에 적합한 경우 Broadcast Hash Join을 사용하는 것을 고려하세요. 이 방식은 작은 RDD를 각 워커 노드로 전달한 후, 큰 RDD의 각 파티션과 맵-사이드 결합(map-side combining)을 수행합니다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pySpark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" broadcast\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# DataFrame")]),t._v("\njoined "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" large_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("broadcast"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("smaller_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" right_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" left_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" how"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'inner'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# SparkSQL")]),t._v("\nSELECT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" BROADCAST"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" \nFROM t1 INNER JOIN t2 ON t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h2",{attrs:{id:"shuffle-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shuffle-join"}},[t._v("#")]),t._v(" Shuffle Join")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("셔플 해시 조인(Shuffle Hash Join)")]),t._v(": Shuffle Hash Join은 정렬 없이 두 개의 데이터 프레임을 조인합니다. Spark 실행기(executor)의 메모리에 저장할 수 있는 두 개의 작은 테이블을 조인할 때 적합합니다.")]),t._v(" "),a("li",[a("strong",[t._v("정렬-병합 조인(Sort-Merge Join)")]),t._v(": Sort-Merge Join은 다음 과정을 거쳐 조인을 수행합니다:  데이터 프레임을 셔플(Shuffle) → 데이터를 정렬(Sort) → 조인 수행. 이 방식은 대규모 테이블 조인에 적합하지만, 메모리 부족(OOM) 문제와 성능 저하를 초래할 수 있습니다.  이런 경우, 버킷화(Bucketing)를 통해 조인의 효율성을 높일 수 있습니다.")])]),t._v(" "),a("h2",{attrs:{id:"bucketing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bucketing"}},[t._v("#")]),t._v(" Bucketing")]),t._v(" "),a("p",[t._v("버킷화는 조인 키를 기준으로 데이터를 미리 셔플하고 정렬한 후 중간 테이블에 저장합니다. 이를 통해 대규모 테이블 조인 시 셔플과 정렬 비용을 줄일 수 있습니다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 버킷화 테이블 생성")]),t._v("\ndf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'orderId'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'orderDate'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("saveAsTable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bucketedOrder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"조인-전-재파티셔닝-repartition-before-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#조인-전-재파티셔닝-repartition-before-join"}},[t._v("#")]),t._v(" 조인 전 재파티셔닝(Repartition before Join)")]),t._v(" "),a("p",[t._v("두 RDD가 동일한 키와 동일한 파티셔닝 코드로 파티셔닝되어 있다면, 조인해야 할 RDD 레코드가 동일한 워커 노드에 위치할 가능성이 높아집니다. 이는 셔플 활동과 데이터 불균형(skewness)을 줄여 성능을 향상시킬 수 있습니다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("return_ord "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returns"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repartition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\norder_rep "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" order"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repartition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("N"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\njoined "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" return_ord"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("order_rep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'product_id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=r.exports}}]);