(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{340:function(t,a,s){"use strict";s.r(a);var n=s(27),e=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"예제로-살펴보는-ai를-위한-데이터-엔지니어링"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예제로-살펴보는-ai를-위한-데이터-엔지니어링"}},[t._v("#")]),t._v(" 예제로 살펴보는 AI를 위한 데이터 엔지니어링")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://blog.det.life/data-engineering-for-ai-practical-examples-and-best-practices-d40f9832f8dc?gi=b2932bdb277e",target:"_blank",rel:"noopener noreferrer"}},[t._v("Data Engineering for AI: Practical Examples and Best Practices"),a("OutboundLink")],1),t._v(" 를 읽고 정리했습니다.")]),t._v(" "),a("p",[t._v("AI 기반 애플리케이션이 점점 더 널리 사용됨에 따라, AI를 위한 데이터 관리도 전문적인 기술과 강력한 프레임워크를 요구하는 복잡한 작업으로 발전했습니다. AI 데이터 엔지니어링을 예제를 통해 함께 알아봅니다.")]),t._v(" "),a("h2",{attrs:{id:"_1-ai-워크로드를-위한-데이터-파이프라인-구축"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-ai-워크로드를-위한-데이터-파이프라인-구축"}},[t._v("#")]),t._v(" 1. AI 워크로드를 위한 데이터 파이프라인 구축")]),t._v(" "),a("p",[t._v("AI를 위한 데이터 파이프라인은 유연성과 확장성이 필요하며, 종종 배치(batch)와 스트리밍(streaming) 기능을 모두 요구합니다. Apache Spark와 Kafka를 사용하여 데이터를 처리하는 파이프라인 예제를 살펴보겠습니다.")]),t._v(" "),a("h3",{attrs:{id:"예시-apache-spark와-kafka를-활용한-실시간-데이터-수집"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예시-apache-spark와-kafka를-활용한-실시간-데이터-수집"}},[t._v("#")]),t._v(" 예시: Apache Spark와 Kafka를 활용한 실시간 데이터 수집")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" from_json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("types "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StructType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" DoubleType  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define the schema for incoming data  ")]),t._v("\nschema "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StructType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  \n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"timestamp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" DoubleType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Initialize Spark session  ")]),t._v("\nspark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("appName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"AI Data Pipeline"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Read streaming data from Kafka  ")]),t._v("\ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("readStream \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"kafka"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("option"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"kafka.bootstrap.servers"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"localhost:9092"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("option"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"subscribe"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data-topic"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Parse JSON data  ")]),t._v("\nparsed_df "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("from_json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cast"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"string"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" schema"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data.*"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Apply transformations for AI model  ")]),t._v("\ntransformed_df "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parsed_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("withColumn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"feature"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Example feature engineering  ")]),t._v("\n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Write to data sink  ")]),t._v("\nquery "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transformed_df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writeStream \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"console"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("outputMode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"append"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \nquery"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("이 예시에서는:")]),t._v(" "),a("ul",[a("li",[t._v("Kafka 토픽에서 Spark로 데이터를 가져와 변환 작업을 수행합니다.")]),t._v(" "),a("li",[t._v("간단한 피쳐 변환(feature transformation)을 적용하여 새로운 피쳐 열(feature column)을 생성하며, 이는 이후 단계에서의 AI 모델에 유용할 수 있습니다.")]),t._v(" "),a("li",[t._v("처리된 데이터는 콘솔, 데이터 레이크, 또는 실시간으로 AI 모델에 직접 출력할 수 있습니다.")])]),t._v(" "),a("h2",{attrs:{id:"_2-데이터-품질-및-관측성"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-데이터-품질-및-관측성"}},[t._v("#")]),t._v(" 2. 데이터 품질 및 관측성")]),t._v(" "),a("p",[t._v("AI를 위해 데이터 품질을 보장하는 것은 매우 중요하며, 자동화된 품질 검사를 설정하면 오류를 크게 줄일 수 있습니다. Great Expectations를 사용하여 AI 모델로 데이터가 전달되기 전에 유입 데이터를 검증하는 예를 살펴보겠습니다.")]),t._v(" "),a("blockquote",[a("p",[t._v("great expectations는 python을 기반으로한 데이터 품질 평가 오픈소스 입니다. https://github.com/great-expectations/great_expectations 를 참고하세요")])]),t._v(" "),a("h3",{attrs:{id:"예시-great-expectations를-활용한-데이터-검증"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예시-great-expectations를-활용한-데이터-검증"}},[t._v("#")]),t._v(" 예시: Great Expectations를 활용한 데이터 검증")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" great_expectations"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batch "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BatchRequest  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" great_expectations"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_context "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataContext  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Initialize Great Expectations context  ")]),t._v("\ncontext "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define the data batch to validate  ")]),t._v("\nbatch_request "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BatchRequest"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  \n    datasource_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_datasource"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    data_connector_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_data_connector"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    data_asset_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_table"')]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define a new expectation suite  ")]),t._v("\nsuite "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" context"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_expectation_suite"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ai_data_quality_suite"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" overwrite_existing"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Add data expectations  ")]),t._v("\nvalidator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" context"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_validator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("batch_request"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" expectation_suite_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ai_data_quality_suite"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \nvalidator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("expect_column_values_to_not_be_null"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"timestamp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \nvalidator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("expect_column_values_to_be_in_set"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"status"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"active"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"inactive"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Run validation and check results  ")]),t._v("\nresults "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" validator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("validate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" results"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"success"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Data quality validation failed!"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Data quality validation passed!"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("이 예시에서는:")]),t._v(" "),a("ul",[a("li",[t._v("Great Expectations를 사용하여 데이터의 주요 특성들을 검증합니다. 예를 들어, 비어 있지 않은 타임스탬프 값이나 상태 열(status column)에서 허용 가능한 상태를 확인합니다.")]),t._v(" "),a("li",[t._v("이러한 검증은 AI 모델 학습 파이프라인에 데이터 이상값이 유입되기 전에 문제를 발견하는 데 도움을 줍니다.")]),t._v(" "),a("li",[t._v("Great Expectations를 일반적인 예로 제시했지만, Splunk, SignalFx 또는 자체 개발한 도구와 같은 다른 프레임워크/도구를 선택할 수도 있습니다.")])]),t._v(" "),a("h2",{attrs:{id:"_3-데이터-카탈로그-및-메타데이터-관리"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-데이터-카탈로그-및-메타데이터-관리"}},[t._v("#")]),t._v(" 3. 데이터 카탈로그 및 메타데이터 관리")]),t._v(" "),a("p",[t._v("풍부한 메타데이터를 포함한 데이터 카탈로그를 생성하면 데이터 과학자가 사용하는 데이터의 계보(lineage), 품질, 맥락(context)을 이해하는 데 도움을 줍니다. Apache Atlas를 사용하면 메타데이터를 프로그래밍 방식으로 카탈로그화할 수 있습니다.")]),t._v(" "),a("h3",{attrs:{id:"예시-apache-atlas-api를-활용한-데이터-카탈로그화"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예시-apache-atlas-api를-활용한-데이터-카탈로그화"}},[t._v("#")]),t._v(" 예시: Apache Atlas API를 활용한 데이터 카탈로그화")]),t._v(" "),a("p",[t._v("Apache Atlas는 데이터셋, 테이블, 계보를 나타내는 엔티티를 생성하는 등 메타데이터를 관리할 수 있는 REST API를 제공합니다.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" requests  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define the entity details for a new dataset  ")]),t._v("\nentity "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"entities"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  \n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"typeName"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hive_table"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"attributes"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sales_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"qualifiedName"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sales_data@prod"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"description"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Sales data for AI model training"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"owner"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data_engineering_team"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tableType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MANAGED_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"columns"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  \n                    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"timestamp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"date"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sale_amount"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"double"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Send a POST request to create the entity  ")]),t._v("\nresponse "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" requests"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("post"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"http://atlas-server:21000/api/atlas/v2/entity"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    headers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Content-Type"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"application/json"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("entity"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("status_code "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Entity created in Apache Atlas"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Failed to create entity: ')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"')])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define the entity details for a new dataset  ")]),t._v("\nentity "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"entities"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  \n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"typeName"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hive_table"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"attributes"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sales_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"qualifiedName"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sales_data@prod"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"description"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Sales data for AI model training"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"owner"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"data_engineering_team"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tableType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MANAGED_TABLE"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"columns"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  \n                    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"timestamp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"date"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sale_amount"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataType"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"double"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n                "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  \n            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[t._v("이 예시에서는:")]),t._v(" "),a("ul",[a("li",[t._v("sales_data 테이블을 스키마, 소유자, 목적에 대한 메타데이터로 카탈로그화하여 AI 학습에 사용되는 데이터를 추적하고 이해하기 쉽게 만듭니다.")])]),t._v(" "),a("h2",{attrs:{id:"_4-확장성을-위한-데이터-파티셔닝-및-인덱싱"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-확장성을-위한-데이터-파티셔닝-및-인덱싱"}},[t._v("#")]),t._v(" 4. 확장성을 위한 데이터 파티셔닝 및 인덱싱")]),t._v(" "),a("p",[t._v("데이터 파티셔닝과 인덱싱은 특히 분산 시스템에서 성능을 향상시킬 수 있습니다. Delta Lake를 사용한 파티셔닝과 인덱싱 예제를 살펴보겠습니다.")]),t._v(" "),a("h3",{attrs:{id:"예시-delta-lake를-활용한-파티셔닝과-인덱싱"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예시-delta-lake를-활용한-파티셔닝과-인덱싱"}},[t._v("#")]),t._v(" 예시: Delta Lake를 활용한 파티셔닝과 인덱싱")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" delta"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tables "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DeltaTable  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession  \n  \nspark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("appName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Delta Lake Example"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\  \n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Load data and write with partitioning  ")]),t._v("\ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"s3://my-bucket/data.csv"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \ndf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"delta"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("partitionBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"date"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/delta/sales_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Optimize and create Z-Order index on relevant columns  ")]),t._v("\ndelta_table "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DeltaTable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("forPath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/delta/sales_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \ndelta_table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("executeZOrderBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"customer_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("이 예시에서는:")]),t._v(" "),a("ul",[a("li",[t._v("sales_data를 날짜별로 파티셔닝하여 데이터 스캔 크기를 줄여 쿼리 성능을 향상시킵니다.")]),t._v(" "),a("li",[t._v("customer_id에 Z-Order 인덱싱을 적용하여 해당 열에 대한 읽기 성능을 최적화하고, downstream AI 프로세스(예: 고객 맞춤형 모델)를 더 빠르게 만듭니다.")])]),t._v(" "),a("h2",{attrs:{id:"_5-데이터-마스킹-및-익명화"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-데이터-마스킹-및-익명화"}},[t._v("#")]),t._v(" 5. 데이터 마스킹 및 익명화")]),t._v(" "),a("p",[t._v("민감한 데이터를 처리할 때 익명화는 필수적입니다. 다음 예시는 Python의 Faker 라이브러리를 사용하여 원본 데이터셋의 구조와 분포를 유지하면서 합성된 익명화된 데이터를 생성하는 방법을 보여줍니다.")]),t._v(" "),a("h3",{attrs:{id:"예시-python의-faker를-사용한-데이터-마스킹"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#예시-python의-faker를-사용한-데이터-마스킹"}},[t._v("#")]),t._v(" 예시: Python의 Faker를 사용한 데이터 마스킹")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" faker "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Faker  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd  \n  \nfake "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Faker"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"customer_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fake"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uuid4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"customer_name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fake"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n    "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transaction_amount"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("fake"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random_number"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("digits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" _ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n  \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Display anonymized data  ")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("p",[t._v("이 예시에서는:")]),t._v(" "),a("ul",[a("li",[t._v("Faker가 고객 ID를 위한 가짜 UUID, 합성된 이름, 임의의 거래 금액을 생성합니다.")]),t._v(" "),a("li",[t._v("이 익명화된 데이터셋은 고객 개인정보를 위험에 빠뜨리지 않고 AI 모델 학습과 테스트에 안전하게 사용할 수 있습니다.")]),t._v(" "),a("li",[t._v("Faker를 일반적인 예로 사용했지만, PySyft, Presidio, SDV 등과 같은 다양한 마스킹 라이브러리가 있습니다. 사용자 정의 함수를 구축할 수도 있습니다.")])]),t._v(" "),a("h2",{attrs:{id:"결론"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#결론"}},[t._v("#")]),t._v(" 결론")]),t._v(" "),a("p",[t._v("AI를 위한 데이터 관리에는 전통적인 데이터 엔지니어링 기술과 AI 에서 발생하는 문제를 처리하기 위한 전문 기술이 결합됩니다. 스트리밍 수집, 데이터 검증, 카탈로그화, 파티셔닝, 마스킹과 같은 예시를 통해 이러한 기술들이 어떻게 AI 워크로드를 지원하는지 알 수 있습니다. AI를 위한 데이터 엔지니어링의 분야는 방대하며, AI가 발전함에 따라 이러한 도구와 모범 사례에 대한 숙련도를 유지하는 것이 중요합니다.")])])}),[],!1,null,null,null);a.default=e.exports}}]);